{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.加载训练的data pipeline 但不训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from util.Trainer import Trainer\n",
    "\n",
    "from util.dataloader import ChemDataRaw, MyTokenizer, get_data_loader, ChemDataSet\n",
    "from util.TrainInits import init_seed, print_model_parameters\n",
    "import configparser\n",
    "from torch import nn \n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "init_seed(114514)\n",
    "\n",
    "# config\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config-small.conf\")\n",
    "vocab_file = config['data']['vocab_file']\n",
    "from_exist = bool(eval(config['train']['from_exist']))\n",
    "\n",
    "\n",
    "device = config['model']['device']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12788it [00:00, 127867.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100001it [00:00, 203013.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing /home/data/wd/correct_norm_reaction.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4883729it [00:45, 106774.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing /home/data/wd/correct_norm_retro_no_ca.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1801113it [00:23, 75739.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100001 4883729 1801113\n",
      "10000 4883729 1801113\n",
      "-------------- showing examples --------------------------------\n",
      "molecules:\n",
      "[2.023179999999999, 59.37, 0.8235513388578103, 2.2812516757370815] O=C(CCc1ccccc1)NCCCN1CCNCC1.CN1CCN(CCC(C)(C)NC(=O)CCc2ccc(C#N)cc2)CC1!\n",
      "reactions:\n",
      "[1.45928, 157.45, 0.5691895676039367, 2.5280768445981483] CCCCOC(=O)c1nc(C#N)c2ccc(S(=O)(=O)c3ccccc3)cc2c1O.NCC(=O)O>>O=C(c1nc(C#N)c2ccc(S(=O)(=O)c3ccccc3)cc2c1O)NCC(=O)O!\n",
      "retro:\n",
      "[4.994900000000005, 60.440000000000005, 0.4500855369554003, 4.574026652089674] CC(=O)OC(C)=O.F[C@H]1[C@H]2[C@@H]3CCC(=O)C=C3C[C@@H](CCCCCO)[C@@H]2[C@H]2CCC(=O)[C@@]2(C)C1<<F[C@H]1[C@H]2[C@@H]3CCC(=O)C=C3C[C@@H](CCCCCOC(C)=O)[C@@H]2[C@H]2CCC(=O)[C@@]2(C)C1!\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset\n",
    "tokenizer = MyTokenizer(vocab_file)\n",
    "config['data']['vocab_size'] = str(tokenizer.get_vocab_size())\n",
    "mol_files = config['data']['mol_files'].split(',')\n",
    "react_files = config['data']['react_files'].split(',')\n",
    "retro_files = config['data']['retro_files'].split(',')\n",
    "chemdata = ChemDataRaw(mol_files, react_files, retro_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_set = ChemDataSet(chemdata.train_data, tokenizer, int(config['model']['n_positions']))\n",
    "# props, train_str, attention_mask = train_set[0]\n",
    "# print(props.shape, train_str.shape, attention_mask.shape)\n",
    "# props, train_str, attention_mask = train_set[1]\n",
    "# print(props.shape, train_str.shape, attention_mask.shape)\n",
    "# exit()\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loader(chemdata, \n",
    "                                                        int(config['train']['batch_size']), \n",
    "                                                        tokenizer,\n",
    "                                                        int(config['model']['n_positions'])-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.gpt2_copy import generate_model\n",
    "if from_exist:\n",
    "    model = torch.load(os.path.join(config['train']['log_dir'], 'best_model.pth'))\n",
    "else:\n",
    "    model = generate_model(config)\n",
    "model = model.to(device)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    # else:\n",
    "    #     nn.init.uniform_(p)\n",
    "# print_model_parameters(model, only_num=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (props, seqs, attention_mask) in enumerate(train_loader):\n",
    "    props = props.to(device)\n",
    "    seqs = seqs.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = [seqs, props]\n",
    "    data = [props, seqs[..., :-1]]\n",
    "\n",
    "    output = model(data, labels, attention_mask)\n",
    "    # print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = '!'\n",
    "eos_ids = tokenizer.char2num[eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_input就在batch里面\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = seqs[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = attention_mask[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = attn_mask[attn_mask == 1]\n",
    "input_ids = input_ids[input_ids != 68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.], device='cuda:3')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids 和 attn_mask就是输出，统一成encoded_input字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input={}\n",
    "encoded_input['input_ids'] = input_ids\n",
    "encoded_input['attention_mask'] = attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([58, 46, 58, 39, 23,  8, 23, 23, 39, 58, 59, 23, 39, 26, 58, 59, 23, 39,\n",
       "         58, 59, 23,  8, 59, 49, 26, 35, 51, 58, 39, 58, 39, 49, 26, 59, 26, 58,\n",
       "         58, 59, 58, 39, 49, 26, 59, 26, 58, 58, 34, 34, 51, 39, 58, 39, 58, 39,\n",
       "         49, 26, 59, 26, 58, 58, 59, 58, 39, 49, 26, 59, 26, 58, 58, 59, 58, 39,\n",
       "         23,  8, 23, 23, 39, 58, 59, 23, 39, 26, 58, 59, 23, 39, 58, 59, 23,  8,\n",
       "         59, 49, 26], device='cuda:3'),\n",
       " 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.], device='cuda:3')}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_configs = {\n",
    "    'beam_size': 1, # test greedy search\n",
    "    'max_gen_len': 50,\n",
    "    'end_ids': 68,\n",
    "    'pad_id': 68,\n",
    "    'no_repeat_ngram_size': 1,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.8,\n",
    "    'top_k': 10,\n",
    "    'top_p': 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (ChemGPT) is not compatible with `.generate()`, as it doesn't have a language model head.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1295\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03mGenerates sequences of token ids for models with a language modeling head. The method supports the following\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03m['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;66;03m# 0. Validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_kwargs(model_kwargs\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# 1. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:973\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[1;32m    972\u001b[0m     exception_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 973\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[0;31mTypeError\u001b[0m: The current model class (ChemGPT) is not compatible with `.generate()`, as it doesn't have a language model head."
     ]
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChemGPT(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(71, 384)\n",
       "    (wpe): Embedding(1024, 384)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (prop_in_linear): Linear(in_features=4, out_features=384, bias=True)\n",
       "  (prop_out_linear): Linear(in_features=384, out_features=4, bias=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=71, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N/C(=N\\\\OC(=O)c1ccccc1Cl)c1nccnc1Oc1cccc(C(F)(F)F)c1\\n', 'COc1ccc(CCN2Cc3c(N=Cc4ccc(O)cc4O)cccc3C2=O)cc1OC\\n', 'C/C(=N\\\\OCC(=O)O/N=C(\\\\N)COc1ccc(Cl)cc1Cl)c1ccc(Cl)cc1\\n', 'CCCCCCCC/C=C\\\\CCCCCCCC(=O)OC[C@H](O)COC[C@H](O)CO\\n', 'O=S(OCC(F)(F)C(F)(F)C(F)(F)F)OCC(F)(F)C(F)(F)C(F)(F)F\\n']\n"
     ]
    }
   ],
   "source": [
    "mol_files = r'/home/data/wd/zinc_data.txt'\n",
    "\n",
    "with open('/home/data/wd/zinc_data.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    first_few_lines = lines[:5]  # Change the number 5 to the desired number of lines\n",
    "    print(first_few_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/Zhouyu/MODEL/code/zinc250k.csv', usecols=['smiles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/Zhouyu/MODEL/code/zinc250k.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\\n', 'C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1\\n', 'N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)cc2)cc1\\n', 'CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c2CCCCC3)C1\\n', 'N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#N)C12CCCCC2\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('/home/Zhouyu/MODEL/code/zinc250k.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    first_few_lines = lines[:5]  # Change the number 5 to the desired number of lines\n",
    "    print(first_few_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
